
<!DOCTYPE html>
<html lang="en">

<br>
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  
    <title>Jielin Qiu</title>
    
    <meta name="author" content="Jielin Qiu">
    <meta name="viewport" content="width=900">
    <!--<link rel="stylesheet" type="text/css" href="stylesheet.css">-->
    <!--<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">-->
    <link rel="stylesheet" type="text/css" href="css/font.css">
    <link rel="stylesheet" type="text/css" href="css/main.css">
    <link rel="icon" type="image/png" href="images/cmulogo.png">
  
    <!-- <script src=”main.js” defer></script> -->
</head>


<body>



<table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
        <tr>
            <td style="padding:2.5%;width:25%;max-width:40%">
              <img style="width:80%;max-width:100%" alt="profile photo" src="images/Jielin_2023.jpg">
            </td>
            <td width="77%" valign="left">
				<h1>Jielin Qiu</h1>
                Ph.D., Computer Science Department<br>
				Carnegie Mellon University<br>
				Advisors: <a href="https://lileicc.github.io/">Prof. Lei Li</a> and <a href="https://www.cs.cmu.edu/~christos/">Prof. Christos Faloutsos</a><br>
				</p>

                <p style="text-align:left">
                <a href="mailto:jielinqiu2024@gmail.com">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=2khNwjoAAAAJ">Google Scholar</a> &nbsp/&nbsp
                <a href="https://github.com/Jason-Qiu">Github</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/jielin-qiu-60876b122/">LinkedIn</a>
                  </p>
                </p>
            </td>
        </tr>
        </table>
    </td>
    </tr>
    </table>

<!--
    <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
        <tr>
        <td>
            <h2> Update </h2>
            <hr>
    
                  <li> 
                      <p style="color:rgb(255, 94, 0)">I'm on the job market and I'm looking for a Research Scientist/Applied Scientist position starting from 2024. <br>Please feel free to contact me if you have any opening or if there's a suitable match!</p>
                  </li>
        </td>
        </tr>
    </table>
 -->


    <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
        <tr>
        <td>
            <h2> About </h2>
            <hr>
    
                  <!--<li> -->
              <p>
              My research interests lie in Multimodal Machine Learning. The central goal of my research is to design scalable inference and learning algorithms to connect language, perception, and control for robust multimodal learning. 
			  I strive to achieve this by learning the unique modality equivalence through abstract multimodal representations.
              My current research lies in the foundations of multimodal learning with applications in multimedia, computer vision, natural language processing, healthcare, and embodied AI. 
              </p>
			  
              <p>
              I received my Ph.D. from the Computer Science Department at School of Computer Science, Carnegie Mellon University. I was extremely fortunate to be advised by <a href="https://lileicc.github.io/">Prof. Lei Li</a> and <a href="https://www.cs.cmu.edu/~christos/">Prof. Christos Faloutsos</a>.
              Before that, I received my B.Eng. from Shanghai Jiao Tong University, advised by <a href="https://scholar.google.com/citations?user=709il6EAAAAJ&hl=en">Prof. Bao-Liang Lu</a>.
              I've worked as a research intern at Google, Meta, Microsoft, Amazon Web Services, and Adobe. My research was generously supported by CMU CSD fellowships and fundings from DARPA, NSF, Adobe, Allegheny Health Network, and Cleveland Clinic.
              </p>
			  <!--</li> -->
    
        </td>
        </tr>
    </table>


    <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
        <tr>
        <td>
            <h2> News </h2>
            <hr>
			
    
                <ul style="list-style-type: none; padding: 2px; overflow-y: scroll; height:400px;">
                  <li><font color="#777777">[2024-04]</font> <a href="http://arxiv.org/abs/2403.04735">SnapNTell</a> gets accepted by EMNLP 2024 Findings. </li>                
                  <li><font color="#777777">[2024-04]</font> <a href="http://arxiv.org/abs/2306.04216">MMSum dataset</a> gets accepted by CVPR 2024 as Poster Highlight (Top 11.9%). Check our <a href="https://mmsum-dataset.github.io/"><strong>MMSum dataset</strong></a>!</li>
                  <li><font color="#777777">[2024-03]</font> <a href="https://arxiv.org/abs/2306.05696">Embodied Policy Learning with Language-based Scene Summarization</a>  gets accepted by NAACL 2024. </li>
                  <li><font color="#777777">[2024-01]</font> <a href="https://arxiv.org/abs/2212.08044">MMRobustness</a> gets accepted as the very first paper at Journal of Data-centric Machine Learning Research (DMLR) 2024. Check our <a href="https://mmrobustness.github.io/"><strong>MMRobustness benchmark</strong></a>!</li>
                  <li><font color="#777777">[2023-11]</font> One paper about <a href="https://arxiv.org/abs/2304.06286">Cardiovascular record retrieval</a> gets accepted by PMLR ML4H 2023.</li>
				  <li><font color="#777777">[2023-10]</font> Start a research internship at Google.</li>
                  <li><font color="#777777">[2023-10]</font> One paper about <a href="https://arxiv.org/abs/2208.06348">human languages and brain signals</a> gets accepted by EMNLP Findings 2023.</li>
                  <li><font color="#777777">[2023-06]</font> One <a href="https://arxiv.org/abs/2306.05696">paper</a> accepted as spotlight by ICML 2023 Workshop on Interactive Learning with Implicit Human Feedback.</li>
                  <li><font color="#777777">[2023-06]</font> Two papers accepted by ICML 2023 Workshop on Machine Learning for Multimodal Healthcare Data.</li>
                  <li><font color="#777777">[2023-05]</font> Start a research internship at Meta.</li>
                  <li><font color="#777777">[2023-05]</font> One paper about <a href="https://arxiv.org/abs/2210.04722">multimodal summarization by Optimal Transport</a> gets accepted by ACL Findings 2023.</li>
                  <li><font color="#777777">[2023-04]</font> One paper about data augmentation on Geodesics gets accepted by ICML 2023.</li>
                  <li><font color="#777777">[2023-04]</font> Invited talk at Microsoft Research Cambridge.</li>
                  <li><font color="#777777">[2023-02]</font> One paper accepted by CVPR 2023.</li>
                  <li><font color="#777777">[2023-02]</font> One paper accepted by ICASSP 2023.</li>
                  <li><font color="#777777">[2023-01]</font> Start a research internship at Microsoft.</li>
                  <li><font color="#777777">[2023-01]</font> One paper accepted by EACL Findings 2023.</li>
                  <li><font color="#777777">[2023-01]</font> One paper accepted by AISTATS 2023.</li>
                  <li><font color="#777777">[2022-10]</font> One paper accepted by WACV 2023.</li>
				  <li><font color="#777777">[2022-10]</font> One paper accepted by NeurIPS 2022 Workshop on Distribution Shifts.</li>
                  <li><font color="#777777">[2022-10]</font> Top Reviewers in NeurIPS 2022.</li>
                  <li><font color="#777777">[2022-06]</font> One paper accepted by MLHC 2022.</li>
                  <li><font color="#777777">[2022-05]</font> Start a research internship at AWS AI.</li>
                  <li><font color="#777777">[2022-05]</font> One paper accepted by ICML 2022 workshop on Principles of Distribution Shift.</li>
                  <li><font color="#777777">[2022-04]</font> One paper accepted by ICLR 2022 Workshop on Socially Responsible Machine Learning.</li>
                  <li><font color="#777777">[2021-09]</font> Receive a gift funding from Adobe. Thanks, Adobe!</li>
                  <li><font color="#777777">[2021-05]</font> Start a research internship at Adobe research.</li>
                  
                </ul>
    
        </td>
        </tr>
    </table>


    <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
        <tr>
        <td>
            <h2> Work Experience </h2>
            <hr>
    
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
                <tr>
                    <td width="100%" valign="middle">
                        <p>
                            <li>[2023/10 - 2024/04] - Google. Student Researcher. <br> Work on multimodal watermark. </li>
                            <li>[2023/05 - 2023/08] - Meta. Research Scientist Intern. <br>Work on entity-centric VQA and retrieval-augmented multimodal LLM.</li>
                            <li>[2023/01 - 2023/04] - Microsoft. Research Intern. <br>Work on multimodal video summarization dataset and entity recognition image dataset.</li>
                            <li>[2022/05 - 2022/12] - Amazon Web Service. Applied Scientist Intern. <br>Work on robustness study of multimodal image-text models under distribution shifts.</li>
                            <li>[2021/05 - 2021/12] - Adobe. Research Intern. <br>Work on multimodal Livesteam video segmentation and summarization.</li>
                        </p>
                    </td>
                </tr>
            </table>
    
        </td>
        </tr>
    </table>


    <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
        <h2> Selected Publications </h2>
        <hr>
			  <p>* marked as equal contribution</p>


              <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
                <tr>
                    <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                        <div class="image-container">
                            <img src="images/MMWatermark.png" width="100%">
                        </div>
                    </td>
                    <td style="width:65%; vertical-align:middle">
                        <papertext>
                            <papertitle>Evaluating Durability: Benchmark Insights into Multimodal Watermarking</papertitle>
                            <br>
                        <strong>Jielin Qiu*</strong>,
                        <a href="https://willxxy.github.io/">William Han*</a>,
                      <a href="https://xuandongzhao.github.io/">Xuandong Zhao</a>,
                      <a href="https://scholar.google.com/citations?user=R-gy8CQAAAAJ&hl=en">Shangbang Long</a>,
                      <br>
                      <a href="https://www.cs.cmu.edu/~christos/">Christos Faloutsos</a>,
                      <a href="https://lileicc.github.io/">Lei Li</a>
                        <br>
                        Under Review
                        <br>
                        <a href="https://arxiv.org/abs/2406.03728">[paper]</a>
                        </papertext>
                    </td>
                </tr>
            </table>
            <br>


        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/SnapNTell_intro.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>SnapNTell: Enhancing Entity-Centric Visual Question Answering with Retrieval Augmented Multimodal LLM</papertitle>
                        <br>
                    <strong>Jielin Qiu</strong>,
                  <a href="https://andreamad8.github.io/">Andrea Madotto</a>,
                  <a href="https://zlinao.github.io/">Zhaojiang Lin</a>,
                  <a href="https://www.pacrook.net/">Paul Crook</a>,
                  <a href="https://scholar.google.com/citations?user=e0KdvxUAAAAJ&hl">Ethan Xu</a>,
				  <br>
                  <a href="https://lunadong.com/">Luna Dong</a>,
                  <a href="https://www.cs.cmu.edu/~christos/">Christos Faloutsos</a>,
                  <a href="https://lileicc.github.io/">Lei Li</a>,
                  <a href="https://www.linkedin.com/in/babakd/">Babak Damavandi</a>,
                  <a href="https://shanemoon.com/">Seungwhan Moon</a>
                    <br>
                    EMNLP 2024 Findings
                    <br>
                    <a href="http://arxiv.org/abs/2403.04735">[paper]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>

        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/SUM_APM.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>Embodied Executable Policy Learning with Language-based Scene Summarization</papertitle>
                        <br>
                    <strong>Jielin Qiu*</strong>,
                    <a href="https://mxu34.github.io/">Mengdi Xu*</a>,
                    <a href="https://willxxy.github.io/">William Han*</a>,
                    <a href="https://shanemoon.com/">Seungwhan Moon</a>,
                    <a href="https://safeai-lab.github.io/">Ding Zhao</a>
                    <br>
                    NAACL 2024 <br>ICML 2023 Workshop on Interactive Learning with Implicit Human Feedback (spotlight)
                    <br>
                    <a href="https://arxiv.org/abs/2306.05696">[paper]</a>
					<a href="https://github.com/Jason-Qiu/Embodied_Policy_Learning">[code]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>


        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/mmsum.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>MMSum: A Dataset for Multimodal Summarization and Thumbnail Generation of Videos</papertitle>
                        <br>
                    <strong>Jielin Qiu</strong>,
                  <a href="https://jiachengzhuml.github.io/">Jiacheng Zhu</a>,
                  <a href="https://willxxy.github.io/">William Han</a>,
                  <a href="">Aditesh Kumar</a>,
                  <a href="">Karthik Mittal</a>,
                  <a href="">Claire Jin</a>,
                  <a href="https://zyang-ur.github.io/">Zhengyuan Yang</a>,
                  <a href="https://scholar.google.com/citations?user=WR875gYAAAAJ&hl=en">Linjie Li</a>,
                  <a href="https://scholar.google.com/citations?user=vJWEw_8AAAAJ&hl=en">Jianfeng Wang</a>,
                  <a href="https://safeai-lab.github.io/">Ding Zhao</a>,
				  <br>
                  <a href="https://aisecure.github.io/">Bo Li</a>,
                  <a href="https://scholar.google.com/citations?user=cDcWXuIAAAAJ&hl=en">Lijuan Wang</a>
                    <br>
                    CVPR 2024 (Poster Highlight 11.9%)
                    <br>
                    <a href="http://arxiv.org/abs/2306.04216">[paper]</a>
					<a href="https://mmsum-dataset.github.io/">[website]</a>
					<a href="https://mmsum-dataset.github.io/">[dataset]</a>
					<a href="https://mmsum-dataset.github.io/">[code]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>


        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/mm_robustness.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>Benchmarking Robustness of Multimodal Image-Text Models under Distribution Shift</papertitle>
                        <br>
                    <strong>Jielin Qiu</strong>,
                  <a href="https://bryanyzhu.github.io/">Yi Zhu</a>,
                  <a href="https://sxjscience.github.io/">Xingjian Shi</a>,
                  <a href="http://florianwenzel.com/">Florian Wenzel</a>,
                  <a href="https://sites.google.com/site/zhiqiangtanghomepage/home">Zhiqiang Tang</a>,
                  <a href="https://safeai-lab.github.io/">Ding Zhao</a>,
				  <br>
                  <a href="https://aisecure.github.io/">Bo Li</a>,
                  <a href="http://www.cs.cmu.edu/~muli/">Mu Li</a>
                    <br>
                    Journal of Data-centric Machine Learning Research (DMLR) 2024
                    <br>
                    <a href="https://arxiv.org/abs/2212.08044">[paper]</a>
					<a href="https://mmrobustness.github.io/">[website]</a>
					<a href="https://github.com/Jason-Qiu/MM_Robustness">[code]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>




        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/Entity6K.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>Entity6K: A Large Open-Domain Evaluation Dataset for Real-World Entity Recognition</papertitle>
                        <br>
                    <strong>Jielin Qiu</strong>,
                  <a href="https://willxxy.github.io/">William Han</a>,
                  <a href="">Winfred Wang</a>,
                  <a href="https://zyang-ur.github.io/">Zhengyuan Yang</a>,
                  <a href="https://scholar.google.com/citations?user=WR875gYAAAAJ&hl=en">Linjie Li</a>,
                  <a href="https://scholar.google.com/citations?user=vJWEw_8AAAAJ&hl=en">Jianfeng Wang</a>,
                  <a href="https://www.cs.cmu.edu/~christos/">Christos Faloutsos</a>,
                  <a href="https://lileicc.github.io/">Lei Li</a>,
                  <a href="https://scholar.google.com/citations?user=cDcWXuIAAAAJ&hl=en">Lijuan Wang</a>
                    <br>
                    Under Review
                    <br>
                    <a href="https://arxiv.org/abs/2403.12339">[paper]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>



        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/SCCS.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>Semantics-Consistent Cross-domain Summarization via Optimal Transport Alignment</papertitle>
                        <br>
                    <strong>Jielin Qiu</strong>,
                  <a href="https://jiachengzhuml.github.io/">Jiacheng Zhu</a>,
                  <a href="https://mxu34.github.io/">Mengdi Xu</a>,
                  <a href="https://research.adobe.com/person/franck-dernoncourt/">Franck Dernoncourt</a>,
                  <a href="https://sites.google.com/site/trungbuistanford/">Trung Bui</a>,
                  <a href="https://research.adobe.com/person/zhaowen-wang/">Zhaowen Wang</a>,
                  <a href="https://aisecure.github.io/">Bo Li</a>,
                  <a href="https://safeai-lab.github.io/">Ding Zhao</a>,
                  <a href="https://sites.google.com/view/hailinjin">Hailin Jin</a>
                    <br>
                    ACL 2023 Findings
                    <br>
                    <a href="https://arxiv.org/abs/2210.04722">[paper]</a>
					<a href="https://research.adobe.com/publication/sccs-semantics-consistent-cross-domain-summarization-via-optimal-transport-alignment/">[press]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>




        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/brain_topo.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>Can Brain Signals Reveal Inner Alignment with Human Languages?</papertitle>
                        <br>
                  <a href="https://willxxy.github.io/">William Han*</a>,
                    <strong>Jielin Qiu*</strong>,
                  <a href="https://jiachengzhuml.github.io/">Jiacheng Zhu</a>,
                  <a href="https://mxu34.github.io/">Mengdi Xu</a>,
                  <a href="https://www.meche.engineering.cmu.edu/directory/bios/weber-douglas.html">Douglas Weber</a>,
				  <br>
                  <a href="https://aisecure.github.io/">Bo Li</a>,
                  <a href="https://safeai-lab.github.io/">Ding Zhao</a>
                    <br>
                    EMNLP 2023 Findings
                    <br>
                    <a href="https://arxiv.org/abs/2208.06348">[paper]</a>
					<a href="https://github.com/Jason-Qiu/EEG_Language_Alignment">[code]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>


        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/ECG_encoding.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>Automated Cardiovascular Record Retrieval by Multimodal Learning between Electrocardiogram and Clinical Report</papertitle>
                        <br>
                    <strong>Jielin Qiu*</strong>,
                  <a href="https://jiachengzhuml.github.io/">Jiacheng Zhu*</a>,
                  <a href="https://www.linkedin.com/in/shiqiliu2/">Shiqi Liu</a>,
                  <a href="https://willxxy.github.io/">William Han</a>,
                  <a href="">Jingqi Zhang</a>,
                  <a href="https://www.linkedin.com/in/chaojing-duan-0b3266127/">Chaojing Duan</a>,
                  <a href="https://scholar.google.com/citations?hl=en&user=o0Y0GLcAAAAJ">Michael Rosenberg</a>,
                  <a href="https://www.linkedin.com/in/emerson-liu-950479/">Emerson Liu</a>,
                  <a href="https://scholar.google.com/citations?user=7iFu3a4AAAAJ">Douglas Weber</a>,
                  <a href="https://safeai-lab.github.io/">Ding Zhao</a>
                    <br>
                    PMLR Proceedings of Machine Learning for Health 2023
                    <br>
                    <a href="https://arxiv.org/abs/2304.06286">[paper]</a>
					<a href="https://github.com/Jason-Qiu/ECG_image_encoding">[code]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>

        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/CMR.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>Multimodal Representation Learning of Cardiovascular Magnetic Resonance Imaging</papertitle>
                        <br>
                    <strong>Jielin Qiu*</strong>,
                  <a href="https://peidehuang.github.io/">Peide Huang*</a>,
                  <a href="https://scholar.google.com/citations?user=sEaPG-YAAAAJ&hl=en">Makiya Nakashima</a>,
                  <a href="https://www.linkedin.com/in/jaehyun-lee-455b101a3/">Jaehyun Lee</a>,
                  <a href="https://jiachengzhuml.github.io/">Jiacheng Zhu</a>,
                  <a href="https://www.lerner.ccf.org/cardiovascular-metabolic/tang/">Wilson Tang</a>,
                  <a href="https://my.clevelandclinic.org/staff/23589-po-hao-chen">Pohao Chen</a>,
                  <a href="https://my.clevelandclinic.org/staff/29515-christopher-nguyen">Christopher Nguyen</a>,
                  <a href="https://scholar.google.com/citations?user=ysNh3pMAAAAJ&hl=en">Byung-Hak Kim</a>,
				  <br>
                  <a href="https://my.clevelandclinic.org/staff/14119-deborah-kwon">Debbie Kwon</a>,
                  <a href="https://scholar.google.com/citations?user=7iFu3a4AAAAJ">Douglas Weber</a>,
                  <a href="https://safeai-lab.github.io/">Ding Zhao</a>,
                  <a href="https://scholar.google.com/citations?user=6kWCeW0AAAAJ&hl=en">David Chen</a>
                    <br>
                    ICML 2023 Workshop on Machine Learning for Multimodal Healthcare Data
                    <br>
                    <a href="https://arxiv.org/abs/2304.07675">[paper]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>

        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/ECG_transfer.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>Transfer Knowledge from Natural Language to Electrocardiography: Can We Detect Cardiovascular Disease Through Language Models?</papertitle>
                        <br>
                    <strong>Jielin Qiu*</strong>,
                  <a href="https://willxxy.github.io/">William Han*</a>,
                  <a href="https://jiachengzhuml.github.io/">Jiacheng Zhu</a>,
                  <a href="https://mxu34.github.io/">Mengdi Xu</a>,
                  <a href="https://scholar.google.com/citations?hl=en&user=o0Y0GLcAAAAJ">Michael Rosenberg</a>,
                  <a href="https://www.linkedin.com/in/emerson-liu-950479/">Emerson Liu</a>,
                  <a href="https://www.meche.engineering.cmu.edu/directory/bios/weber-douglas.html">Douglas Weber</a>,
                  <a href="https://safeai-lab.github.io/">Ding Zhao</a>
                    <br>
                    EACL 2023 Findings
                    <br>
                    <a href="https://aclanthology.org/2023.findings-eacl.33.pdf">[paper]</a>
					<a href="https://github.com/Jason-Qiu/Transfer_Knowledge_from_Language_to_Electrocardiography">[code]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>

        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/icassp_ECG.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>Cardiac Disease Diagnosis on Imbalanced Electrocardiography Data Through Optimal Transport Augmentation</papertitle>
                        <br>
                    <strong>Jielin Qiu*</strong>,
                  <a href="https://jiachengzhuml.github.io/">Jiacheng Zhu</a>*,
                  <a href="https://mxu34.github.io/">Mengdi Xu</a>,
                  <a href="https://peidehuang.github.io/">Peide Huang</a>,
                  <a href="https://scholar.google.com/citations?hl=en&user=o0Y0GLcAAAAJ">Michael Rosenberg</a>,
                  <a href="https://www.meche.engineering.cmu.edu/directory/bios/weber-douglas.html">Douglas Weber</a>,
                  <a href="https://www.linkedin.com/in/emerson-liu-950479/">Emerson Liu</a>,
                  <a href="https://safeai-lab.github.io/">Ding Zhao</a>
                    <br>
                    ICASSP 2023
                    <br>
                    <a href="https://ieeexplore.ieee.org/abstract/document/10095562">[paper]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>
		
        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/LiveSeg.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>LiveSeg: Unsupervised Multimodal Temporal Segmentation of Long Livestream Videos</papertitle>
                        <br>
                    <strong>Jielin Qiu</strong>,
                  <a href="https://research.adobe.com/person/franck-dernoncourt/">Franck Dernoncourt</a>,
                  <a href="https://sites.google.com/site/trungbuistanford/">Trung Bui</a>,
                  <a href="https://research.adobe.com/person/zhaowen-wang/">Zhaowen Wang</a>,
                  <a href="https://safeai-lab.github.io/">Ding Zhao</a>,
                  <a href="https://sites.google.com/view/hailinjin">Hailin Jin</a>
                    <br>
                    WACV 2023
                    <br>
                    <a href="https://openaccess.thecvf.com/content/WACV2023/papers/Qiu_LiveSeg_Unsupervised_Multimodal_Temporal_Segmentation_of_Long_Livestream_Videos_WACV_2023_paper.pdf">[paper]</a>
					<a href="https://research.adobe.com/publication/liveseg-unsupervised-multimodal-temporal-segmentation-of-long-livestream-videos/">[press]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>
		
		
        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/geodesics.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>Interpolation for Robust Learning: Data Augmentation on Geodesics</papertitle>
                        <br>
                  <a href="https://jiachengzhuml.github.io/">Jiacheng Zhu</a>,
                    <strong>Jielin Qiu</strong>,
                  <a href="http://www-personal.umich.edu/~aritra/">Aritra Guha</a>,
				  <a href="https://lucas110550.github.io/about/">Zhuolin Yang</a>,
				  <a href="https://dept.stat.lsa.umich.edu/~xuanlong/">XuanLong Nguyen</a>,
				  <br>
                  <a href="https://aisecure.github.io/">Bo Li</a>,
                  <a href="https://safeai-lab.github.io/">Ding Zhao</a>
                    <br>
                    ICML 2023
                    <br>
                    <a href="https://arxiv.org/abs/2302.02092">[paper]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>
		
        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/A2Summ.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>Align and Attend: Multimodal Summarization with Dual Contrastive Losses</papertitle>
                        <br>
                  <a href="https://boheumd.github.io/">Bo He</a>,
                  <a href="https://junwang.umiacs.io/">Jun Wang</a>, 
                    <strong>Jielin Qiu</strong>,
                  <a href="https://www.cs.umd.edu/~abhinav/">Abhinav Shrivastava</a>,
                  <a href="https://sites.google.com/site/trungbuistanford/">Trung Bui</a>,
                  <a href="https://research.adobe.com/person/zhaowen-wang/">Zhaowen Wang</a>
                    <br>
                    CVPR 2023
                    <br>
                    <a href="https://arxiv.org/abs/2303.07284">[paper]</a>
					<a href="https://github.com/boheumd/A2Summ">[code]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>
		



        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/robustness_workshop.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>Benchmarking Robustness under Distribution Shift of Multimodal Image-Text Models</papertitle>
                        <br>
                    <strong>Jielin Qiu</strong>,
                  <a href="https://bryanyzhu.github.io/">Yi Zhu</a>,
                  <a href="https://sxjscience.github.io/">Xingjian Shi</a>,
                  <a href="https://sites.google.com/site/zhiqiangtanghomepage/home">Zhiqiang Tang</a>,
                  <a href="https://safeai-lab.github.io/">Ding Zhao</a>,
                  <a href="https://aisecure.github.io/">Bo Li</a>,
                  <a href="http://www.cs.cmu.edu/~muli/">Mu Li</a>
                    <br>
                    NeurIPS 2022 Workshop on Distribution Shifts
                    <br>
                    <a href="https://openreview.net/forum?id=PN4CUHbnlc">[paper]</a>
					<a href="https://www.amazon.science/publications/benchmarking-robustness-under-distribution-shift-of-multimodal-image-text-models">[press]</a>
					<a href="https://github.com/Jason-Qiu/MM_Robustness">[code]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>
		


        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/GeoECG.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>GeoECG: Data Augmentation via Wasserstein Geodesic Perturbation for Robust Electrocardiogram Prediction</papertitle>
                        <br>
                  <a href="https://jiachengzhuml.github.io/">Jiacheng Zhu*</a>,
                    <strong>Jielin Qiu*</strong>,
                  <a href="https://lucas110550.github.io/about/">Zhuolin Yang</a>,
                  <a href="https://scholar.google.com/citations?user=7iFu3a4AAAAJ">Douglas Weber</a>,
				  <br>
                  <a href="https://scholar.google.com/citations?hl=en&user=o0Y0GLcAAAAJ">Michael Rosenberg</a>,
                  <a href="https://www.linkedin.com/in/emerson-liu-950479/">Emerson Liu</a>,
                  <a href="https://aisecure.github.io/">Bo Li</a>,
                  <a href="https://safeai-lab.github.io/">Ding Zhao</a>
                    <br>
                    MLHC 2022
                    <br>
                    <a href="https://proceedings.mlr.press/v182/zhu22a.html">[paper]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>
		
		
        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/group_DRL.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>Group Distributionally Robust Reinforcement Learning with Hierarchical Latent Variables</papertitle>
                        <br>
                  <a href="https://mxu34.github.io/">Mengdi Xu</a>,
                  <a href="https://peidehuang.github.io/">Peide Huang</a>,
                  <a href="https://yaruniu.com/">Yaru Niu</a>,
                  <a href="https://scholar.google.com/citations?user=bAn6bEYAAAAJ&hl=en">Visak Kumar</a>,
                    <strong>Jielin Qiu</strong>,
                  <a href="https://www.linkedin.com/in/chao-fang-8832621a">Chao Fang</a>,
                  <a href="https://sites.google.com/site/kuanhuilee0201">Kuan-Hui Lee</a>,
                  <a href="https://scholar.google.com/citations?user=pOA6uKMAAAAJ&hl=en">Xuewei Qi</a>,
                  <a href="http://www.columbia.edu/~khl2114/">Henry Lam</a>,
                  <a href="https://aisecure.github.io/">Bo Li</a>,
                  <a href="https://safeai-lab.github.io/">Ding Zhao</a>
                    <br>
                    AISTATS 2023
                    <br>
                    <a href="https://arxiv.org/pdf/2210.12262.pdf">[paper]</a>
					<a href="https://github.com/mxu34/GDR-RL">[code]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>
		

		
        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/ECG_aug.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>Data Augmentation via Wasserstein Geodesic Perturbation for Robust Electrocardiogram Prediction</papertitle>
                        <br>
                  <a href="https://jiachengzhuml.github.io/">Jiacheng Zhu*</a>,
                    <strong>Jielin Qiu*</strong>,
                  <a href="https://lucas110550.github.io/about/">Zhuolin Yang</a>,
                  <a href="https://scholar.google.com/citations?hl=en&user=o0Y0GLcAAAAJ">Michael Rosenberg</a>,
                  <a href="https://www.linkedin.com/in/emerson-liu-950479/">Emerson Liu</a>,
                  <a href="https://aisecure.github.io/">Bo Li</a>,
                  <a href="https://safeai-lab.github.io/">Ding Zhao</a>
                    <br>
                    ICLR 2022 Workshop on Socially Responsible Machine Learning (SRML)
                    <br>
                    <a href="https://download.huan-zhang.com/events/srml2022/accepted/zhu22data.pdf">[paper]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>
		
        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/IEEE_CDS.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>Comparing Recognition Performance and Robustness of Multimodal Deep Learning Models for Multimodal Emotion Recognition</papertitle>
                        <br>
				  <a href="https://scholar.google.co.jp/citations?user=Gdg7z24AAAAJ&hl=en">Wei Liu</a>,
                    <strong>Jielin Qiu</strong>,
                  <a href="https://weilongzheng.github.io/">Wei-Long Zheng</a>,
                  <a href="https://bcmi.sjtu.edu.cn/~blu/">Bao-Liang Lu</a>
                    <br>
                    IEEE Transactions on Cognitive and Developmental Systems 2021
                    <br>
                    <a href="https://ieeexplore.ieee.org/abstract/document/9395500">[paper]</a>
					<a href="https://github.com/Jason-Qiu/DCCA_Emotion_Recognition">[code]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>
		
        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/HPNet.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>Visual Sequence Learning in Hierarchical Prediction Networks and Primate Visual Cortex</papertitle>
                        <br>
                    <strong>Jielin Qiu</strong>,
                  <a href="https://www.linkedin.com/in/ge-huang-779769104">Ge Huang</a>,
                  <a href="https://www.cnbc.cmu.edu/~tai/">Tai Sing Lee</a>
                    <br>
                    NeurIPS 2019
                    <br>
                    <a href="https://proceedings.neurips.cc/paper/2019/file/08040837089cdf46631a10aca5258e16-Paper.pdf">[paper]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>
		
        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/EEG_sex.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>Investigating Sex Differences in Classification of Five Emotions from EEG and Eye Movement Signals</papertitle>
                        <br>
				    Lan-Qing Bao,
                    <strong>Jielin Qiu</strong>,
                  <a href="https://haotang1995.github.io/">Hao Tang</a>,
                  <a href="https://weilongzheng.github.io/">Wei-Long Zheng</a>,
				  <a href="https://bcmi.sjtu.edu.cn/~blu/">Bao-Liang Lu</a>
                    <br>
                    EMBC 2019
                    <br>
                    <a href="https://ieeexplore.ieee.org/document/8857476">[paper]</a>
					<a href="https://github.com/Jason-Qiu/DCCA_Emotion_Recognition">[code]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>
		
        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/AAAI_RLG.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>
                        <papertitle>Approximation Gradient Error Variance Reduced Optimization</papertitle>
                        <br>
				  <a href="https://scholar.google.com/citations?user=P-79KOcAAAAJ&hl=en">Weiye Zhao</a>,
				  <a href="https://scholar.google.com/citations?user=bih9IIwAAAAJ&hl=en">Yang Liu</a>,
				  <a href="https://scholar.google.com/citations?user=tDyRAbkAAAAJ&hl=en">Xiaoming Zhao</a>,
                    <strong>Jielin Qiu</strong>,
                  <a href="https://jianpeng.web.engr.illinois.edu/">Jian Peng</a>
                    <br>
                    AAAI-RLG 2019
                    <br>
                    <a href="http://aaai-rlg.mlanctot.info/papers/AAAI19-RLG-Paper04.pdf">[paper]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>
		
        <table width="900" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td style="width:35%; vertical-align:middle; padding-right: 20px;">
                    <div class="image-container">
                        <img src="images/DCCA_ICONIP.png" width="100%">
                    </div>
                </td>
                <td style="width:65%; vertical-align:middle">
                    <papertext>Multi-view Emotion Recognition Using Deep Canonical Correlation Analysis</papertitle>
                        <br>
                    <strong>Jielin Qiu</strong>,
                  <a href="https://scholar.google.co.jp/citations?user=Gdg7z24AAAAJ&hl=en">Wei Liu</a>,
				  <a href="https://scholar.google.com/citations?user=709il6EAAAAJ&hl=en">Bao-Liang Lu</a>
                    <br>
                    ICONIP 2018
                    <br>
                    <a href="https://bcmi.sjtu.edu.cn/~blu/papers/2018/Qiu2018Multi-viewEmotion.pdf">[paper]</a>
					<a href="https://github.com/Jason-Qiu/DCCA_Emotion_Recognition">[code]</a>
                    </papertext>
                </td>
            </tr>
        </table>
        <br>
















    <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
        <tr>
        <td>
            <h2> Services </h2>
            <hr>
    
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
                <tr>
                    <td width="100%" valign="middle">
                        <p>
                            <li>Reviewer: ICML 2021-2024, CVPR 2022-2024, WACV 2023-2024, ICLR 2023-2024, ICASSP 2023-2024, ECCV 2022-2024, ACL Rolling Review (ARR) 2024, ACCV 2024, CHIL 2022-2024, NeurIPS 2022-2023, ICCV 2023, KDD 2023, EACL 2023, MICCAI 2023, AISTATS 2023, MLHC 2022-2023, ACM MM 2022.</li>
                            <li>PC Member: AAAI 2021-2024, ACL 2023, EMNLP 2022-2023.</li>
                            <li>Journal Reviewer: Transactions on Pattern Analysis and Machine Intelligence (TPAMI), Transactions on Machine Learning Research (TMLR), Journal of Data-centric Machine Learning Research (DMLR), IEEE Transactions on Neural Networks and Learning Systems.</li>
                            <li>Committee: NeurIPS 2022 virtual deep-dive session chair, CMU RISS Committee.</li>
                            <br>
                        </p>
                    </td>
                </tr>
            </table>
            
        </td>
        </tr>
    </table>


    <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
        <h2> Teaching </h2>
        <hr>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="0">
            <tr>
                <td width="100%" valign="middle">
                    <p>
                        <li>Teaching Assistant of CMU 16-824 Visual Learning and Recognition, Instructor: <a href="https://www.cs.cmu.edu/~junyanz/">Prof. Jun-Yan Zhu</a>, Fall 2021</li>
                        <li>Teaching Assistant of CMU 11-777 MultiModal Machine Learning, Instructor: <a href="https://yonatanbisk.com/index.html">Prof. Yonatan Bisk</a>, Spring 2021</li>
                        <br>
                    </p>
                </td>
            </tr>
        </table>

    </td>
    </tr>
    </table>
    

    <table width="900" border="0" align="center" cellspacing="0" cellpadding="0">
        <tr>
        <td>
    
            <p align="right">
                Website template from <a href="https://github.com/jonbarron/website">Jon Barron</a> and <a href="https://mxu34.github.io/">Mengdi Xu</a>.
            </p>
            
        </td>
        </tr>
    </table>
    <br>

</body>

</html>